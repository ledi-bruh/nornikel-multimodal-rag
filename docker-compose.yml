services:

  qdrant:
    container_name: qdrant
    image: qdrant/qdrant:latest
    restart: always
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    environment:
      - QDRANT__SERVICE__HOST=0.0.0.0
    configs:
      - source: qdrant_config
        target: /qdrant/config/production.yaml
    volumes:
      - ./data/qdrant:/qdrant/storage
    networks:
      - nornikel-net

  # lmdeploy:
  #   container_name: lmdeploy
  #   image: openmmlab/lmdeploy:latest
  #   ports:
  #     - 23333:23333
  #   volumes:
  #     - ${LOCAL_HUGGINGFACE_MODELS}:/root/.cache/huggingface
  #   stdin_open: true
  #   tty: true
  #   ipc: host
  #   command: lmdeploy serve api_server OpenGVLab/InternVL2-8B-AWQ --backend turbomind --model-format awq
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: "all"
  #             capabilities: [gpu]

  nlu:
    container_name: nlu
    build:
      context: .
      dockerfile: ./Dockerfile
    restart: always
    volumes:
      - ${LOCAL_HUGGINGFACE_MODELS}:/opt/.cache/huggingface
    ports:
      - ${APP_PORT}:${APP_PORT}
    environment:
      - HF_HOME=/opt/.cache/huggingface
    env_file:
      - .env
    depends_on:
      - qdrant
    networks:
      - nornikel-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

configs:
  qdrant_config:
    content: |
      log_level: INFO

networks:
  nornikel-net:
    driver: bridge
